# SCANNER: FLUJO COMPLETO POST-FILTRADO
## An√°lisis Exhaustivo - Qu√© hace Scanner despu√©s de filtrar

**Fecha**: 3 Noviembre 2025  
**Archivo**: `services/scanner/scanner_engine.py`

---

## üéØ PUNTO DE PARTIDA

Scanner ejecut√≥ `_process_snapshots_optimized()` y tiene:

```python
scored_tickers = [
    ScannerTicker(symbol="CRBU", price=1.51, rvol=2493.73, change_percent=204.55, score=2500, rank=1),
    ScannerTicker(symbol="MSPR", price=5.20, rvol=2787.5, change_percent=122.64, score=2200, rank=2),
    ... (335 tickers m√°s)
    # Total: 337 tickers filtrados y rankeados
]
```

Ahora sigue **7 PASOS CR√çTICOS**...

---

## PASO 1: LIMITAR RESULTADOS (l√≠neas 129-131)

```python
if len(scored_tickers) > settings.max_filtered_tickers:
    scored_tickers = scored_tickers[:settings.max_filtered_tickers]
```

**CONFIGURACI√ìN**:
- `max_filtered_tickers` = 1000 (settings.py)

**L√ìGICA**:
```
Si filtered = 337:  ‚Üí No hace nada (337 < 1000)
Si filtered = 1,500: ‚Üí Recorta a 1,000
Si filtered = 50:    ‚Üí No hace nada (50 < 1000)
```

**POR QU√â**:
- Evitar sobrecarga de memoria
- Limitar procesamiento posterior (categorizaci√≥n, BD)
- Solo los top N por score contin√∫an

**RESULTADO**: 337 tickers (sin cambios)

---

## PASO 2: GUARDADO EN 3 CACH√âS (l√≠neas 134-143)

Scanner guarda los tickers filtrados en **3 ubicaciones diferentes**:

### 2.1 CACH√â EN MEMORIA (l√≠neas 135-137)

```python
self.last_filtered_tickers = scored_tickers
self.last_filtered_time = datetime.now()
```

**UBICACI√ìN**: Atributos de la clase `ScannerEngine`

**CARACTER√çSTICAS**:
- ‚úÖ Acceso instant√°neo (0ms)
- ‚úÖ Sin operaciones I/O
- ‚ùå Se pierde si servicio se reinicia
- ‚ùå No compartido con otros servicios

**USADO POR**:
- API endpoint `GET /api/filtered` (consultas r√°pidas)
- Comparaciones internas del scanner
- No persiste entre reinicios

**EJEMPLO**:
```python
# Otro lugar del c√≥digo puede hacer:
latest_tickers = scanner_engine.last_filtered_tickers
# Retorna los 337 tickers inmediatamente
```

---

### 2.2 CACH√â EN REDIS - COMPLETO (l√≠neas 140 ‚Üí funci√≥n 750-783)

```python
await self._save_filtered_tickers_to_cache(scored_tickers)
```

**IMPLEMENTACI√ìN** (`_save_filtered_tickers_to_cache`, l√≠neas 750-783):

```python
# Clave din√°mica por sesi√≥n
cache_key = f"scanner:filtered_complete:{self.current_session.value}"
# Ejemplo: "scanner:filtered_complete:PRE_MARKET"

# Serializar TODOS los tickers completos
tickers_data = [ticker.model_dump(mode='json') for ticker in tickers]

# Guardar en Redis con TTL
await self.redis.set(
    cache_key,
    tickers_data,
    ttl=60,  # 60 segundos
    serialize=True
)
```

**ESTRUCTURA EN REDIS**:
```json
Key: scanner:filtered_complete:PRE_MARKET
TTL: 60 segundos
Value: [
  {
    "symbol": "CRBU",
    "timestamp": "2025-11-03T05:30:00Z",
    "price": 1.51,
    "bid": 1.50,
    "ask": 1.52,
    "volume": 13856866,
    "volume_today": 13856866,
    "open": 0.50,
    "high": 1.60,
    "low": 0.48,
    "prev_close": 0.50,
    "change": 1.01,
    "change_percent": 204.55,
    "rvol": 2493.73,
    "rvol_slot": 2493.73,
    "price_from_high": -5.63,
    "price_from_low": 108.33,
    "market_cap": 5000000,
    "float_shares": 3000000,
    "sector": "Healthcare",
    "industry": "Biotechnology",
    "exchange": "NASDAQ",
    "score": 2500.5,
    "rank": 1,
    "filters_matched": ["rvol_high", "price_range"],
    "session": "PRE_MARKET",
    "metadata": {
      "gaps": {...},
      "gap_size_classification": "EXTREME",
      ...
    }
  },
  ... 336 tickers m√°s
]
```

**CARACTER√çSTICAS**:
- ‚úÖ Persiste 60 segundos
- ‚úÖ Compartido entre instancias
- ‚úÖ Tickers COMPLETOS con todos los campos
- ‚úÖ Se refresca cada scan (cada 30s)

**USADO POR**:
- Otros servicios que necesitan acceder a tickers filtrados
- API Gateway para consultas
- Debugging y monitoring

**KEYS POR SESI√ìN**:
```
scanner:filtered_complete:PRE_MARKET
scanner:filtered_complete:MARKET_OPEN
scanner:filtered_complete:POST_MARKET
scanner:filtered_complete:CLOSED
```

---

### 2.3 CACH√â EN REDIS - SORTED SET (DESACTIVADO pero existe en c√≥digo)

**Funci√≥n** (`_publish_filtered_tickers`, l√≠neas 785-815):

```python
# DESACTIVADO en l√≠nea 146 del flujo principal
# await self._publish_filtered_tickers(scored_tickers)

# Pero el c√≥digo existe:
for ticker in tickers:
    await self.redis.xadd(
        "stream:scanner:filtered",
        {
            "symbol": ticker.symbol,
            "price": ticker.price,
            "score": ticker.score,
            ...
        }
    )
```

**POR QU√â EST√Å DESACTIVADO**:
- Stream sin consumidores (hu√©rfano)
- Ya no se usa despu√©s del refactor
- Comentado en l√≠nea 145-146

---

## PASO 3: CATEGORIZACI√ìN (l√≠nea 143)

```python
await self.categorize_filtered_tickers(scored_tickers)
```

**QU√â HACE**: El proceso M√ÅS COMPLEJO del scanner. Vamos l√≠nea por l√≠nea...

### 3.1 Funci√≥n `categorize_filtered_tickers()` (l√≠neas 928-989)

```python
async def categorize_filtered_tickers(
    self,
    tickers: List[ScannerTicker],
    emit_deltas: bool = True
) -> Dict[str, List[ScannerTicker]]:
```

**ENTRADA**:
- `tickers`: Los 337 tickers filtrados
- `emit_deltas`: True (emite cambios incrementales)

**FLUJO COMPLETO**:

#### 3.1.1 Obtener categor√≠as (l√≠nea 945)

```python
categories = self.categorizer.get_all_categories(tickers, limit_per_category=20)
```

Esto llama a `scanner_categories.py` ‚Üí `get_all_categories()`:

```python
results = {}

for category in ScannerCategory:  # 11 categor√≠as
    ranked = get_category_rankings(tickers, category, limit=20)
    
    if ranked:
        results[category.value] = ranked

return results
```

**PROCESO PARA CADA CATEGOR√çA**:

```python
# EJEMPLO: gappers_up

1. FILTRAR tickers que califican (scanner_categories.py l√≠neas 169-175)
   
   categorized = []
   for ticker in tickers:  # Los 337 filtrados
       categories = categorize_ticker(ticker)  # Retorna ["gappers_up", "winners", ...]
       
       if ScannerCategory.GAPPERS_UP in categories:
           categorized.append(ticker)
   
   # Resultado: 97 tickers con change_percent >= 2.0

2. ORDENAR por criterio espec√≠fico (l√≠neas 186-188)
   
   categorized.sort(key=lambda t: t.change_percent or 0, reverse=True)
   
   # Ordenados:
   # 1. CRBU: +204.55%
   # 2. MSPR: +122.64%
   # 3. RAY: +53.76%
   # ...
   # 97. (√∫ltimo con change >= 2%)

3. LIMITAR a top 20 (l√≠nea 219)
   
   return categorized[:20]
```

**CATEGORIZACI√ìN DE UN TICKER** (l√≠neas 76-156 de scanner_categories.py):

```python
def categorize_ticker(ticker: ScannerTicker) -> List[ScannerCategory]:
    categories = []
    
    # 1. GAPPERS (change desde cierre anterior)
    if change_percent >= 2.0:
        categories.append(GAPPERS_UP)
    elif change_percent <= -2.0:
        categories.append(GAPPERS_DOWN)
    
    # 2. MOMENTUM (solo durante market open)
    if session == MARKET_OPEN and change >= 3.0:
        categories.append(MOMENTUM_UP)
    
    # 3. WINNERS/LOSERS (cambios extremos)
    if change_percent >= 5.0:
        categories.append(WINNERS)
    elif change_percent <= -5.0:
        categories.append(LOSERS)
    
    # 4. ANOMALIES (RVOL extremo)
    if rvol >= 3.0:
        categories.append(ANOMALIES)
    
    # 5. HIGH_VOLUME (alto RVOL)
    if rvol >= 2.0:
        categories.append(HIGH_VOLUME)
    
    # 6. NEW_HIGHS/LOWS (posici√≥n en rango)
    if price_from_high <= 0.5:  # Dentro de 0.5% del high
        categories.append(NEW_HIGHS)
    if price_from_low <= 0.5:
        categories.append(NEW_LOWS)
    
    # 7. REVERSALS (gap tracker)
    if is_reversal:
        categories.append(REVERSALS)
    
    return categories
```

**EJEMPLO REAL - Ticker CRBU**:

```
CRBU: change=+204.55%, rvol=2493.73x, price_from_high=-5.63%

Categor√≠as donde entra:
‚úÖ gappers_up (204.55 >= 2.0)
‚úÖ winners (204.55 >= 5.0)
‚úÖ anomalies (2493.73 >= 3.0)
‚úÖ high_volume (2493.73 >= 2.0)
‚ùå new_highs (5.63% desde high, requiere <= 0.5%)
‚ùå momentum_up (no es MARKET_OPEN)
‚ùå reversals (no cumple condiciones)

Total: 4 categor√≠as
```

**RETORNO DE `get_all_categories()`**:

```python
{
    "gappers_up": [
        ScannerTicker(CRBU, rank=1),
        ScannerTicker(MSPR, rank=2),
        ScannerTicker(RAY, rank=3),
        ... (17 m√°s, total 20)
    ],
    "gappers_down": [... 20 tickers],
    "winners": [
        ScannerTicker(CRBU, rank=1),
        ScannerTicker(MSPR, rank=2),
        ... (18 m√°s, total 20)
    ],
    "anomalies": [... 20 tickers],
    "high_volume": [... 20 tickers],
    "new_highs": [... 15 tickers],
    "new_lows": [... 8 tickers],
    "reversals": [... 3 tickers],
    # No hay momentum_up/down (solo en MARKET_OPEN)
}
```

---

#### 3.1.2 GENERAR Y EMITIR DELTAS (l√≠neas 948-973)

```python
for category_name, new_ranking in categories.items():
    # PASO A: Obtener ranking anterior
    old_ranking = self.last_rankings.get(category_name, [])
    
    if not old_ranking:
        # PRIMERA VEZ - Emitir snapshot completo
        logger.info(f"üì∏ First time for {category_name}, emitting snapshot")
        await self.emit_full_snapshot(category_name, new_ranking)
    
    else:
        # ITERACIONES SIGUIENTES - Calcular deltas
        deltas = self.calculate_ranking_deltas(old_ranking, new_ranking, category_name)
        
        if deltas:
            await self.emit_ranking_deltas(category_name, deltas)
    
    # PASO B: Guardar ranking en Redis
    await self._save_ranking_to_redis(category_name, new_ranking)
    
    # PASO C: Actualizar √∫ltimo ranking en memoria
    self.last_rankings[category_name] = new_ranking
```

**FLUJO VISUAL**:

```
ITERACI√ìN 1:
old_rankings = {}
new_rankings = {"gappers_up": [20 tickers]}
‚Üí NO hay old ‚Üí emit_full_snapshot()
‚Üí Guarda en Redis
‚Üí last_rankings["gappers_up"] = [20 tickers]

ITERACI√ìN 2:
old_rankings = {"gappers_up": [A,B,C,D,E...]} (20 tickers)
new_rankings = {"gappers_up": [A,C,E,F,G...]} (20 tickers)
‚Üí S√ç hay old ‚Üí calculate_ranking_deltas()
   ‚Üí Deltas: remove B,D | add F,G | rerank C,E
‚Üí emit_ranking_deltas(deltas)
‚Üí Guarda en Redis
‚Üí last_rankings["gappers_up"] = [A,C,E,F,G...]
```

---

### 3.2 C√ÅLCULO DE DELTAS (l√≠neas 1085-1236)

```python
def calculate_ranking_deltas(
    self,
    old_ranking: List[ScannerTicker],
    new_ranking: List[ScannerTicker],
    category_name: str
) -> List[Dict]:
```

**ALGORITMO COMPLETO**:

#### PASO 3.2.1: Crear mapas de b√∫squeda (l√≠neas 1097-1109)

```python
old_map = {ticker.symbol: ticker for ticker in old_ranking}
new_map = {ticker.symbol: ticker for ticker in new_ranking}

old_symbols = set(old_map.keys())  # {A, B, C, D, E}
new_symbols = set(new_map.keys())  # {A, C, E, F, G}
```

#### PASO 3.2.2: Detectar ADDS (l√≠neas 1111-1126)

```python
added_symbols = new_symbols - old_symbols
# {F, G} - Tickers nuevos

for symbol in added_symbols:
    ticker = new_map[symbol]
    new_rank = next(
        i + 1 for i, t in enumerate(new_ranking) 
        if t.symbol == symbol
    )
    
    deltas.append({
        "type": "add",
        "ticker": ticker.model_dump(mode='json'),
        "rank": new_rank
    })
```

**EJEMPLO**:
```json
{
  "type": "add",
  "ticker": {
    "symbol": "ETHZW",
    "price": 0.05,
    "change_percent": 32.55,
    "rvol": 2.39,
    ...
  },
  "rank": 6
}
```

#### PASO 3.2.3: Detectar REMOVES (l√≠neas 1128-1150)

```python
removed_symbols = old_symbols - new_symbols
# {B, D} - Tickers que salieron

for symbol in removed_symbols:
    deltas.append({
        "type": "remove",
        "ticker": symbol
    })
```

**EJEMPLO**:
```json
{
  "type": "remove",
  "ticker": "LGCL"
}
```

**POR QU√â SALEN TICKERS**:

1. **Cay√≥ su RVOL** < 1.5 ‚Üí No pasa filtro
2. **Ya no tiene volumen** ‚Üí RVOL = None
3. **Cambi√≥ su change_percent** ‚Üí Ya no califica para categor√≠a
4. **Desplazado del top 20** ‚Üí Hay mejores

#### PASO 3.2.4: Detectar UPDATES (l√≠neas 1152-1191)

```python
common_symbols = old_symbols & new_symbols
# {A, C, E} - Tickers en ambos rankings

for symbol in common_symbols:
    old_ticker = old_map[symbol]
    new_ticker = new_map[symbol]
    
    # Verificar si CAMBI√ì alg√∫n dato importante
    if _ticker_data_changed(old_ticker, new_ticker):
        deltas.append({
            "type": "update",
            "ticker": symbol,
            "data": {
                "price": new_ticker.price,
                "volume": new_ticker.volume_today,
                "change_percent": new_ticker.change_percent,
                "rvol": new_ticker.rvol,
                ...
            }
        })
```

**Funci√≥n `_ticker_data_changed()` (l√≠neas 1238-1273)**:

```python
COMPARA CAMPOS CR√çTICOS:

if old_ticker.price != new_ticker.price:
    return True

if old_ticker.volume_today != new_ticker.volume_today:
    return True

if old_ticker.change_percent != new_ticker.change_percent:
    return True

if old_ticker.rvol != new_ticker.rvol:
    return True

# Tambi√©n verifica: bid, ask, high, low, score

return False
```

**EJEMPLO**:
```json
{
  "type": "update",
  "ticker": "CRBU",
  "data": {
    "price": 1.52,        // cambi√≥ de 1.51
    "volume": 14000000,   // cambi√≥ de 13856866
    "rvol": 2520.5,       // cambi√≥ de 2493.73
    "change_percent": 206.0  // cambi√≥ de 204.55
  }
}
```

#### PASO 3.2.5: Detectar RERANKS (l√≠neas 1193-1213)

```python
for symbol in common_symbols:
    old_rank = next(i + 1 for i, t in enumerate(old_ranking) if t.symbol == symbol)
    new_rank = next(i + 1 for i, t in enumerate(new_ranking) if t.symbol == symbol)
    
    if old_rank != new_rank:
        deltas.append({
            "type": "rerank",
            "ticker": symbol,
            "old_rank": old_rank,
            "new_rank": new_rank
        })
```

**EJEMPLO**:
```json
{
  "type": "rerank",
  "ticker": "RAY",
  "old_rank": 3,
  "new_rank": 2
}
```

**POR QU√â CAMBIA EL RANK**:
- Su score aument√≥/disminuy√≥
- Otros tickers superaron/cayeron
- Cambi√≥ su change_percent o rvol

---

### 3.3 EMISI√ìN DE DELTAS (l√≠neas 1275-1333)

```python
async def emit_ranking_deltas(
    self,
    list_name: str,
    deltas: List[Dict]
):
```

**PASO A PASO**:

#### L√≠nea 1291: Incrementar sequence number

```python
self.sequence_numbers[list_name] = self.sequence_numbers.get(list_name, 0) + 1
```

**QU√â ES**:
- Contador incremental por categor√≠a
- Empieza en 0, aumenta en cada emisi√≥n
- Usado para detectar gaps en frontend/websocket

**EJEMPLO**:
```python
Iteraci√≥n 1: gappers_up sequence = 1
Iteraci√≥n 2: gappers_up sequence = 2
Iteraci√≥n 3: gappers_up sequence = 3
...
Iteraci√≥n 120: gappers_up sequence = 120
```

#### L√≠neas 1293-1300: Crear mensaje delta

```python
delta_message = {
    "list": list_name,                        # "gappers_up"
    "sequence": self.sequence_numbers[list_name],  # 120
    "timestamp": datetime.now().isoformat(),  # "2025-11-03T05:30:00Z"
    "changes": len(deltas),                   # 7
    "deltas": json.dumps(deltas)              # "[{type: add, ...}, ...]"
}
```

#### L√≠neas 1303-1309: Publicar al stream

```python
await self.redis.xadd(
    "stream:ranking:deltas",  # ‚Üê Stream que consume WebSocket
    delta_message,
    maxlen=10000  # Mantiene solo √∫ltimos 10,000 deltas
)
```

**ESTRUCTURA EN STREAM**:

```
stream:ranking:deltas:
  [mensaje_id_1] {list: "gappers_up", sequence: 118, deltas: "[...]"}
  [mensaje_id_2] {list: "winners", sequence: 45, deltas: "[...]"}
  [mensaje_id_3] {list: "gappers_up", sequence: 119, deltas: "[...]"}
  [mensaje_id_4] {list: "anomalies", sequence: 102, deltas: "[...]"}
  ...
  [mensaje_id_10000] (l√≠mite alcanzado, se elimina el m√°s viejo)
```

**MAXLEN = 10,000**:
- Mantiene historial de ~1 hora de deltas (si se emite cada 30s)
- Evita crecimiento infinito
- Permite a WebSocket recuperarse si se cae

#### L√≠neas 1312-1327: Logging y estad√≠sticas

```python
adds = sum(1 for d in deltas if d['type'] == 'add')
removes = sum(1 for d in deltas if d['type'] == 'remove')  
updates = sum(1 for d in deltas if d['type'] == 'update')
reranks = sum(1 for d in deltas if d['type'] == 'rerank')

logger.info("‚úÖ Emitted ranking deltas",
    list=list_name,
    sequence=self.sequence_numbers[list_name],
    changes=len(deltas),
    adds=adds,
    removes=removes,
    updates=updates,
    reranks=reranks
)
```

**LOG EJEMPLO**:
```
‚úÖ Emitted ranking deltas: gappers_up, sequence=120, changes=7, adds=1, removes=1, updates=2, reranks=3
```

---

### 3.4 GUARDAR SNAPSHOT DE RANKING EN REDIS (l√≠neas 1346-1391)

```python
await self._save_ranking_to_redis(category_name, new_ranking)
```

**IMPLEMENTACI√ìN**:

```python
async def _save_ranking_to_redis(
    self,
    list_name: str,
    tickers: List[ScannerTicker]
):
    # Convertir tickers a JSON
    ranking_data = [t.model_dump(mode='json') for t in tickers]
    
    # Obtener sequence number actual
    current_sequence = self.sequence_numbers.get(list_name, 0)
    
    # GUARDAR RANKING COMPLETO
    await self.redis.set(
        f"scanner:category:{list_name}",  # "scanner:category:gappers_up"
        json.dumps(ranking_data),
        ttl=3600  # 1 hora
    )
    
    # GUARDAR SEQUENCE NUMBER
    await self.redis.set(
        f"scanner:sequence:{list_name}",  # "scanner:sequence:gappers_up"
        current_sequence,
        ttl=86400  # 24 horas
    )
```

**KEYS EN REDIS**:

```
scanner:category:gappers_up
TTL: 3600s (1 hora)
Value: [
  {"symbol": "CRBU", "price": 1.51, "rvol": 2493.73, ...},
  {"symbol": "MSPR", ...},
  ... (18 m√°s, total 20)
]

scanner:sequence:gappers_up
TTL: 86400s (24 horas)
Value: 120
```

**USADO POR**:
- WebSocket Server para enviar snapshot inicial a nuevos clientes
- Frontend cuando se conecta por primera vez
- Recovery despu√©s de p√©rdida de conexi√≥n

---

### 3.5 Actualizar √∫ltimo ranking en memoria (l√≠nea 973)

```python
self.last_rankings[category_name] = new_ranking
```

**ESTRUCTURA EN MEMORIA**:

```python
self.last_rankings = {
    "gappers_up": [20 ScannerTicker objects],
    "gappers_down": [20 ScannerTicker objects],
    "winners": [20 ScannerTicker objects],
    "anomalies": [20 ScannerTicker objects],
    "high_volume": [20 ScannerTicker objects],
    "new_highs": [15 ScannerTicker objects],
    "new_lows": [8 ScannerTicker objects],
    "reversals": [3 ScannerTicker objects],
}
```

**POR QU√â SE GUARDA**:
- Para comparar en pr√≥xima iteraci√≥n
- Calcular deltas (adds, removes, reranks)
- Evitar recalcular desde Redis

---

## PASO 4: GUARDAR EN TIMESCALEDB (l√≠nea 149)

```python
await self._save_scan_results(scored_tickers)
```

**IMPLEMENTACI√ìN** (l√≠neas 817-860):

```python
async def _save_scan_results(self, tickers: List[ScannerTicker]):
    
    # BATCH INSERT - UNA SOLA QUERY
    batch_data = []
    for ticker in tickers:  # 337 tickers
        metadata_json = json.dumps(ticker.metadata) if ticker.metadata else None
        
        batch_data.append((
            ticker.timestamp,        # time
            ticker.symbol,           # symbol
            ticker.session.value,    # session (PRE_MARKET, MARKET_OPEN, etc.)
            ticker.price,            # price
            ticker.volume,           # volume (instant√°neo)
            ticker.volume_today,     # volume_today (acumulado)
            ticker.change_percent,   # change_percent
            ticker.rvol,             # rvol
            ticker.rvol_slot,        # rvol_slot
            ticker.price_from_high,  # price_from_high
            ticker.price_from_low,   # price_from_low
            ticker.market_cap,       # market_cap
            ticker.float_shares,     # float_shares
            ticker.score,            # score
            ticker.filters_matched,  # filters_matched (array)
            metadata_json            # metadata (jsonb)
        ))
    
    query = """
        INSERT INTO scan_results (
            time, symbol, session, price, volume, volume_today,
            change_percent, rvol, rvol_slot, price_from_high, price_from_low,
            market_cap, float_shares, score, filters_matched, metadata
        )
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16)
    """
    
    await self.db.executemany(query, batch_data)
```

**TABLA `scan_results` EN TIMESCALEDB**:

```sql
CREATE TABLE scan_results (
    time TIMESTAMPTZ NOT NULL,       -- Timestamp del scan
    symbol VARCHAR(10) NOT NULL,      -- Ticker symbol
    session VARCHAR(20),              -- Sesi√≥n de mercado
    price DOUBLE PRECISION,           -- Precio actual
    volume BIGINT,                    -- Volumen instant√°neo
    volume_today BIGINT,              -- Volumen acumulado del d√≠a
    change_percent DOUBLE PRECISION,  -- Cambio %
    rvol DOUBLE PRECISION,            -- Relative Volume
    rvol_slot DOUBLE PRECISION,       -- RVOL del slot actual
    price_from_high DOUBLE PRECISION, -- % desde high
    price_from_low DOUBLE PRECISION,  -- % desde low
    market_cap BIGINT,                -- Market Cap
    float_shares BIGINT,              -- Float
    score DOUBLE PRECISION,           -- Score calculado
    filters_matched TEXT[],           -- Filtros que pas√≥
    metadata JSONB                    -- Datos adicionales (gaps, etc.)
);

-- TimescaleDB: Particionada por tiempo
SELECT create_hypertable('scan_results', 'time');
```

**EJEMPLO DE DATOS GUARDADOS**:

```sql
time                          | symbol | session    | price | volume_today | change_percent | rvol    | score  | rank
------------------------------|--------|------------|-------|--------------|----------------|---------|--------|-----
2025-11-03 05:30:00+00        | CRBU   | PRE_MARKET | 1.51  | 13856866     | 204.55         | 2493.73 | 2500.5 | 1
2025-11-03 05:30:00+00        | MSPR   | PRE_MARKET | 5.20  | 90815645     | 122.64         | 2787.5  | 2200.3 | 2
...
(337 rows para este scan)
```

**PROP√ìSITO**:
- **Hist√≥rico**: Analizar c√≥mo cambi√≥ un ticker en el tiempo
- **Backtesting**: Ver qu√© tickers aparecieron en qu√© momento
- **Analytics**: Calcular estad√≠sticas de rendimiento
- **Auditor√≠a**: Verificar funcionamiento del sistema

**QUERIES T√çPICOS**:

```sql
-- Ver todos los scans de CRBU hoy
SELECT time, price, change_percent, rvol, rank
FROM scan_results
WHERE symbol = 'CRBU'
  AND time >= CURRENT_DATE
ORDER BY time DESC;

-- Ver top gappers de hace 1 hora
SELECT symbol, change_percent, rvol
FROM scan_results
WHERE time >= NOW() - INTERVAL '1 hour'
  AND session = 'PRE_MARKET'
ORDER BY change_percent DESC
LIMIT 20;
```

---

## PASO 5: ACTUALIZAR ESTAD√çSTICAS (l√≠neas 151-157)

```python
elapsed = (time.time() - start) * 1000

self.total_scans += 1
self.total_tickers_scanned += len(enriched_snapshots)  # 11,905
self.total_tickers_filtered += len(scored_tickers)     # 337
self.last_scan_time = datetime.now()
self.last_scan_duration_ms = elapsed  # ~1,500ms
```

**ESTAD√çSTICAS ACUMULADAS**:

```python
{
    "total_scans": 150,                    # Scans desde inicio
    "total_tickers_scanned": 1,785,750,    # 150 √ó 11,905
    "total_tickers_filtered": 50,550,      # 150 √ó 337
    "filter_rate": 2.8%,                   # 337/11,905 = 2.8%
    "last_scan_time": "2025-11-03T05:30:00Z",
    "last_scan_duration_ms": 1500,         # 1.5 segundos
    "current_session": "PRE_MARKET",
    "filters_loaded": 3,
    "filters_enabled": 2,
    "uptime_seconds": 4500                 # 75 minutos
}
```

**ACCESIBLE V√çA**:
- `GET /api/scanner/status` ‚Üí Retorna estas stats

---

## PASO 6: CONSTRUIR RESULTADO (l√≠neas 160-168)

```python
result = ScannerResult(
    timestamp=datetime.now(),
    session=self.current_session,
    total_universe_size=len(enriched_snapshots),  # 11,905
    filtered_count=len(scored_tickers),           # 337
    tickers=scored_tickers,
    filters_applied=[f.name for f in self.filters if f.enabled],
    scan_duration_ms=elapsed
)

return result
```

**OBJETO `ScannerResult`**:

```python
ScannerResult(
    timestamp="2025-11-03T05:30:00.500Z",
    session=MarketSession.PRE_MARKET,
    total_universe_size=11905,
    filtered_count=337,
    tickers=[... 337 ScannerTicker objects ...],
    filters_applied=["rvol_high", "price_range"],
    scan_duration_ms=1500
)
```

Este objeto se retorna al loop principal pero **NO se usa** (es solo para logging).

---

## üì§ RESUMEN: ¬øA D√ìNDE ENV√çA EL SCANNER?

### DESTINOS DE DATOS:

```
Scanner procesa y env√≠a a:

1. MEMORIA (atributos de clase)
   ‚îú‚îÄ last_filtered_tickers ‚Üí 337 tickers completos
   ‚îú‚îÄ last_rankings ‚Üí Dict con 8 categor√≠as √ó 20 tickers
   ‚îî‚îÄ sequence_numbers ‚Üí Dict con sequence por categor√≠a

2. REDIS - CACH√âS (Keys)
   ‚îú‚îÄ scanner:filtered_complete:PRE_MARKET ‚Üí 337 tickers completos
   ‚îú‚îÄ scanner:category:gappers_up ‚Üí Top 20
   ‚îú‚îÄ scanner:category:winners ‚Üí Top 20
   ‚îú‚îÄ scanner:category:anomalies ‚Üí Top 20
   ‚îú‚îÄ scanner:category:high_volume ‚Üí Top 20
   ‚îú‚îÄ scanner:category:new_highs ‚Üí Top 20
   ‚îú‚îÄ scanner:category:new_lows ‚Üí Top 20
   ‚îú‚îÄ scanner:category:gappers_down ‚Üí Top 20
   ‚îú‚îÄ scanner:category:reversals ‚Üí Top 20
   ‚îú‚îÄ scanner:sequence:gappers_up ‚Üí 120
   ‚îú‚îÄ scanner:sequence:winners ‚Üí 45
   ‚îî‚îÄ ... (sequence para cada categor√≠a)

3. REDIS - STREAM (para WebSocket Server)
   ‚îî‚îÄ stream:ranking:deltas ‚Üí Deltas incrementales

4. TIMESCALEDB (para hist√≥rico)
   ‚îî‚îÄ scan_results ‚Üí 337 rows (un scan completo)

5. NO ENV√çA A:
   ‚ùå Otros servicios v√≠a HTTP
   ‚ùå Message queues externos
   ‚ùå Event bus (aunque existe en c√≥digo, no se usa)
```

---

## üîÑ FLUJO COMPLETO RESUMIDO

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ SCANNER: run_scan() - FLUJO COMPLETO                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

1. Lee snapshot:enriched:latest (11,905 tickers)
   ‚îî‚îÄ Verifica timestamp (no reprocesar)

2. Procesa tickers (l√≠neas 127)
   ‚îî‚îÄ Filtra: 11,905 ‚Üí 337 tickers

3. Limita resultados (l√≠neas 129-131)
   ‚îî‚îÄ Max 1,000 (en este caso 337, sin cambios)

4. GUARDA EN 3 CACH√âS:
   
   A. Memoria (l√≠neas 135-137)
      ‚îî‚îÄ self.last_filtered_tickers = [337 tickers]
   
   B. Redis Cache Completo (l√≠nea 140)
      ‚îî‚îÄ scanner:filtered_complete:PRE_MARKET = [337 tickers]
      ‚îî‚îÄ TTL: 60 segundos
   
   C. Redis por Categor√≠as (l√≠nea 143 ‚Üí 948-973)
      Para cada categor√≠a:
      
      i. Categorizar tickers (l√≠nea 945)
         ‚îî‚îÄ gappers_up: 97 de 337 califican
         ‚îî‚îÄ Ordenar por change_percent
         ‚îî‚îÄ Top 20: [CRBU, MSPR, RAY, ...]
      
      ii. Calcular deltas (l√≠nea 959)
          ‚îî‚îÄ Comparar con last_rankings
          ‚îî‚îÄ Genera: [add, remove, update, rerank]
      
      iii. Emitir deltas (l√≠nea 967)
           ‚îî‚îÄ Incrementa sequence: 119 ‚Üí 120
           ‚îî‚îÄ XADD stream:ranking:deltas
      
      iv. Guardar snapshot (l√≠nea 970)
          ‚îî‚îÄ scanner:category:gappers_up = [20 tickers]
          ‚îî‚îÄ scanner:sequence:gappers_up = 120
      
      v. Actualizar memoria (l√≠nea 973)
         ‚îî‚îÄ last_rankings["gappers_up"] = [20 tickers]

5. GUARDAR EN TIMESCALEDB (l√≠nea 149)
   ‚îî‚îÄ INSERT INTO scan_results (337 rows)
   ‚îî‚îÄ Batch insert (una sola query)

6. ACTUALIZAR ESTAD√çSTICAS (l√≠neas 151-157)
   ‚îî‚îÄ total_scans++
   ‚îî‚îÄ total_tickers_scanned += 11,905
   ‚îî‚îÄ total_tickers_filtered += 337

7. RETORNAR RESULTADO (l√≠neas 160-168)
   ‚îî‚îÄ ScannerResult object
   ‚îî‚îÄ No se usa (solo para logging)

TIEMPO TOTAL: ~1.5 segundos
```

---

## üìä DATOS QUE GUARDA POR CATEGOR√çA

Cada categor√≠a tiene:

### 1. Snapshot del Ranking (Redis)

```
Key: scanner:category:gappers_up
TTL: 1 hora
Contiene: Top 20 tickers COMPLETOS

[
  {
    "symbol": "CRBU",
    "timestamp": "2025-11-03T05:30:00Z",
    "price": 1.51,
    "volume_today": 13856866,
    "change_percent": 204.55,
    "rvol": 2493.73,
    "score": 2500.5,
    "rank": 1,
    "metadata": {
      "gaps": {...},
      "gap_size_classification": "EXTREME"
    },
    ... (todos los campos)
  },
  ... 19 m√°s
]
```

### 2. Sequence Number (Redis)

```
Key: scanner:sequence:gappers_up
TTL: 24 horas
Value: 120

Usado para:
- Detectar gaps en frontend
- Sincronizar WebSocket
- Debugging
```

### 3. √öltimo Ranking en Memoria

```python
self.last_rankings["gappers_up"] = [20 ScannerTicker objects]

Usado para:
- Calcular deltas en pr√≥xima iteraci√≥n
- Evitar leer desde Redis
- Comparaci√≥n r√°pida
```

---

## üåä SISTEMA DE DELTAS - AN√ÅLISIS PROFUNDO

### ¬øPOR QU√â DELTAS Y NO SNAPSHOTS SIEMPRE?

**Escenario real**:

```
Iteraci√≥n 1 (05:30:00):
gappers_up = [CRBU, MSPR, RAY, SMX, LGCL, ...]  (20 tickers)
‚Üí WebSocket env√≠a: SNAPSHOT completo (50KB)

Iteraci√≥n 2 (05:30:30):
gappers_up = [CRBU, MSPR, RAY, SMX, ETHZW, ...]  (20 tickers)

Cambios:
- LGCL sali√≥ (rank 5)
- ETHZW entr√≥ (rank 6)
- RAY cambi√≥ de rank 3 a rank 4
- MSPR precio cambi√≥ de 5.20 a 5.25

‚Üí WebSocket env√≠a: DELTA (2KB)
[
  {type: "remove", ticker: "LGCL"},
  {type: "add", ticker: {...}, rank: 6},
  {type: "rerank", ticker: "RAY", old_rank: 3, new_rank: 4},
  {type: "update", ticker: "MSPR", data: {price: 5.25}}
]
```

**VENTAJAS DE DELTAS**:

1. **Bandwidth**: 2KB vs 50KB (25x menos)
2. **Procesamiento Frontend**: Solo actualiza lo que cambi√≥
3. **UX**: Puede animar cambios suavemente
4. **Escalabilidad**: 100 clientes √ó 2KB vs 100 √ó 50KB

**DESVENTAJA**:

Si cliente pierde conexi√≥n o mensajes:
- Puede quedar desincronizado
- **Soluci√≥n**: Sequence numbers + auto-resync

---

## üî¢ SEQUENCE NUMBERS - SISTEMA DE SINCRONIZACI√ìN

### ¬øC√≥mo Funcionan?

```python
# Scanner mantiene un contador por categor√≠a
self.sequence_numbers = {
    "gappers_up": 120,
    "winners": 45,
    "anomalies": 102,
    ...
}

# Cada vez que emite deltas
sequence_numbers["gappers_up"] += 1  # 120 ‚Üí 121

# Se incluye en el mensaje
delta_message = {
    "list": "gappers_up",
    "sequence": 121,
    "deltas": [...]
}
```

### Detecci√≥n de Gaps en Frontend

```typescript
// Frontend mantiene √∫ltimo sequence recibido
lastSequence = {
  "gappers_up": 119
}

// Llega nuevo delta
message = {
  sequence: 121,  // ‚ö†Ô∏è Esperaba 120
  deltas: [...]
}

if (message.sequence !== lastSequence + 1) {
  // GAP DETECTADO (perdi√≥ mensaje 120)
  console.log("‚ö†Ô∏è Gap detected, requesting resync");
  sendMessage({action: "resync", list: "gappers_up"});
}
```

### Recovery Process

```
1. Frontend detecta gap (esperaba 120, recibi√≥ 121)
2. Env√≠a mensaje: {action: "resync", list: "gappers_up"}
3. WebSocket Server recibe resync request
4. Lee scanner:category:gappers_up desde Redis
5. Env√≠a snapshot COMPLETO con sequence actual (121)
6. Frontend reemplaza todos los datos
7. lastSequence = 121
8. Sincronizado ‚úÖ
```

---

## üì° COMUNICACI√ìN CON OTROS SERVICIOS

### ¬øA qu√© servicios env√≠a Scanner?

```
DIRECTAMENTE (via Redis):
‚úÖ WebSocket Server
   ‚îî‚îÄ Lee: stream:ranking:deltas
   ‚îî‚îÄ Lee: scanner:category:{name}
   ‚îî‚îÄ Lee: scanner:sequence:{name}

INDIRECTAMENTE (via Redis cache):
‚úÖ API Gateway (si existe)
   ‚îî‚îÄ Lee: scanner:filtered_complete:{session}
   ‚îî‚îÄ Para endpoints de consulta

‚úÖ Frontend (via WebSocket)
   ‚îî‚îÄ Recibe deltas v√≠a WebSocket Server

ALMACENAMIENTO:
‚úÖ TimescaleDB
   ‚îî‚îÄ Tabla: scan_results
   ‚îî‚îÄ Para hist√≥rico y analytics

NO ENV√çA A:
‚ùå Analytics (Analytics ‚Üí Scanner, no viceversa)
‚ùå Data Ingest (unidireccional)
‚ùå Historical Service
‚ùå Market Session
```

---

## üóÇÔ∏è METADATA - ¬øD√ìNDE SE GUARDA Y USA?

### ¬øScanner guarda metadatas?

**NO**. Scanner **CONSUME** metadatas pero **NO las crea ni modifica**.

**FLUJO DE METADATAS**:

```
Historical Service:
  ‚îî‚îÄ Carga desde Polygon: ticker details, market cap, sector
  ‚îî‚îÄ GUARDA en:
      ‚îú‚îÄ TimescaleDB: ticker_metadata (persistente)
      ‚îî‚îÄ Redis: metadata:ticker:{symbol} (cach√© 24h)

Scanner:
  ‚îî‚îÄ LEE de Redis: metadata:ticker:{symbol}
  ‚îî‚îÄ MGET batch (11,905 metadatas en una operaci√≥n)
  ‚îî‚îÄ USA para:
      ‚îú‚îÄ Filtrar por sector/industry
      ‚îú‚îÄ Enriquecer ScannerTicker
      ‚îî‚îÄ Guardar en scan_results (campo metadata JSONB)
```

**METADATA ENRICHMENT EN SCANNER**:

Cuando Scanner construye un `ScannerTicker`:

```python
ticker = ScannerTicker(
    symbol="CRBU",
    price=1.51,
    rvol=2493.73,
    ...
    sector=metadata.sector,        # De metadata
    industry=metadata.industry,    # De metadata
    market_cap=metadata.market_cap, # De metadata
    float_shares=metadata.float_shares,
    metadata={                     # Metadata ADICIONAL generada
        "gaps": {
            "gap_from_prev_close": 204.55,
            "gap_from_open": -6.25,
            ...
        },
        "gap_size_classification": "EXTREME",
        "gap_metrics": {...}
    }
)
```

**ENTONCES**:

‚úÖ Scanner usa metadatas de Historical  
‚úÖ Scanner AGREGA su propia metadata (gaps, etc.)  
‚úÖ Scanner guarda TODO en scan_results  
‚ùå Scanner NO modifica las metadatas originales

---

## üîÑ ITERACIONES: C√ìMO SE ACUMULA

### Iteraci√≥n 1 (05:30:00)

```
1. Procesa snapshot ‚Üí 337 tickers filtrados
2. Categoriza:
   - gappers_up: 20 tickers (CRBU, MSPR, RAY...)
   - winners: 20 tickers
   - anomalies: 20 tickers
   - ... (8 categor√≠as con datos)

3. NO hay last_rankings ‚Üí Emite SNAPSHOT completo
   ‚îî‚îÄ XADD stream:ranking:deltas: {type: "snapshot", list: "gappers_up", data: [20 tickers]}
   ‚îî‚îÄ Sequence = 1

4. Guarda en Redis:
   ‚îî‚îÄ scanner:category:gappers_up = [20 tickers]
   ‚îî‚îÄ scanner:sequence:gappers_up = 1

5. Guarda en memoria:
   ‚îî‚îÄ last_rankings["gappers_up"] = [20 tickers]
```

### Iteraci√≥n 2 (05:30:30)

```
1. Procesa snapshot ‚Üí 341 tickers filtrados (+4)
2. Categoriza:
   - gappers_up: 20 tickers (CRBU, MSPR, ETHZW, RAY...)

3. S√ç hay last_rankings ‚Üí Calcula DELTAS
   
   Old: [CRBU, MSPR, RAY, SMX, LGCL, ...]
   New: [CRBU, MSPR, ETHZW, RAY, SMX, ...]
   
   Deltas:
   - remove: LGCL (sali√≥ del top 20)
   - add: ETHZW (entr√≥ al top 20)
   - rerank: RAY (3‚Üí4)

4. Emite DELTAS:
   ‚îî‚îÄ XADD stream:ranking:deltas: {sequence: 2, deltas: [remove, add, rerank]}

5. Guarda en Redis:
   ‚îî‚îÄ scanner:category:gappers_up = [20 tickers nuevos] (SOBRESCRIBE)
   ‚îî‚îÄ scanner:sequence:gappers_up = 2 (SOBRESCRIBE)

6. Actualiza memoria:
   ‚îî‚îÄ last_rankings["gappers_up"] = [20 tickers nuevos]
```

### Iteraci√≥n 3 (05:31:00)

```
1. Procesa snapshot ‚Üí 339 tickers (-2)
2. Categoriza ‚Üí gappers_up: 20 tickers
3. Calcula deltas (basado en iteraci√≥n 2)
4. Emite: sequence = 3
5. Guarda y actualiza
```

**OBSERVACI√ìN IMPORTANTE**:

Cada iteraci√≥n es **INDEPENDIENTE** del snapshot de Polygon:
- No depende de qu√© tickers llegaron en iteraciones pasadas
- Siempre procesa el snapshot COMPLETO m√°s reciente
- Rankings se recalculan desde cero cada vez

---

## ‚öôÔ∏è CONFIGURACIONES CR√çTICAS

### TTLs en Redis

```
scanner:filtered_complete:{session}  ‚Üí 60 segundos
  Raz√≥n: Se refresca cada 30s, 60s da margen

scanner:category:{name}              ‚Üí 3600 segundos (1 hora)
  Raz√≥n: Para recovery si WebSocket cae

scanner:sequence:{name}              ‚Üí 86400 segundos (24 horas)
  Raz√≥n: Sequence debe persistir m√°s tiempo

stream:ranking:deltas                ‚Üí maxlen 10,000 mensajes
  Raz√≥n: ~5 horas de historial (si se emite cada 30s)
```

### Por qu√© estos TTLs

**TTL corto (60s)**:
- Datos que cambian r√°pido
- Se regeneran constantemente
- Queremos que expiren si servicio cae

**TTL largo (1h - 24h)**:
- Datos para recovery
- Sequence numbers (no queremos resetear)
- Snapshots para nuevos clientes WebSocket

---

## üîç VERIFICACI√ìN DEL FLUJO

### Comandos para Verificar Cada Paso

```bash
# 1. Ver tickers filtrados completos
docker exec tradeul_redis redis-cli GET "scanner:filtered_complete:PRE_MARKET" | python3 -m json.tool | head -100

# 2. Ver cada categor√≠a
for cat in gappers_up winners anomalies high_volume; do
  echo "=== $cat ==="
  docker exec tradeul_redis redis-cli GET "scanner:category:$cat" | python3 -c "import sys,json; d=json.load(sys.stdin); print(f'Total: {len(d)}'); [print(f'{i+1}. {t[\"symbol\"]}') for i,t in enumerate(d[:5])]"
done

# 3. Ver sequence numbers
docker exec tradeul_redis redis-cli MGET \
  "scanner:sequence:gappers_up" \
  "scanner:sequence:winners" \
  "scanner:sequence:anomalies"

# 4. Ver √∫ltimos deltas emitidos
docker exec tradeul_redis redis-cli XREVRANGE stream:ranking:deltas + - COUNT 5

# 5. Ver datos en TimescaleDB
docker exec tradeul_timescale psql -U tradeul_user -d tradeul -c \
  "SELECT time, symbol, change_percent, rvol, rank FROM scan_results WHERE time > NOW() - INTERVAL '5 minutes' ORDER BY time DESC LIMIT 10;"

# 6. Ver logs del scanner
docker logs tradeul_scanner --tail 100 | grep -E "Emitted ranking deltas|Discovery scan completed"
```

---

## üé≠ EJEMPLO COMPLETO: UN SCAN DE PRINCIPIO A FIN

### T=0: Inicio del Scan

```
05:30:00.000 - Scanner inicia run_scan()
```

### T+100ms: Lee snapshot

```
05:30:00.100 - Lee snapshot:enriched:latest
Resultado: 11,905 tickers del timestamp 2025-11-03T05:29:58Z
```

### T+200ms: Verifica timestamp

```
05:30:00.200 - Compara timestamp
last_snapshot_timestamp = "2025-11-03T05:29:28Z"
nuevo_timestamp = "2025-11-03T05:29:58Z"
‚Üí DIFERENTE ‚Üí Contin√∫a procesamiento
```

### T+300ms - T+1200ms: Procesa snapshots

```
05:30:00.300 - Inicia _process_snapshots_optimized()

  05:30:00.350 - MGET 11,905 metadatas
  Resultado: 5,823 metadatas encontradas
  
  05:30:00.400 - Procesa ticker por ticker
  
  TICKER 1: CRBU
    ‚úì Tiene metadata
    ‚úì Price = 1.51 (> $1)
    ‚úì Volume = 13,856,866 (> 100,000)
    ‚úì RVOL = 2493.73 (> 1.5)
    ‚úì PASA filtros
    ‚Üí Score = 2500.5
    ‚Üí Agrega a filtered_and_scored
  
  TICKER 2: XYZ
    ‚úó No tiene metadata
    ‚Üí Descartado (continue)
  
  TICKER 3: ABC
    ‚úì Tiene metadata
    ‚úì Price = 5.00
    ‚úì Volume = 50,000
    ‚úó RVOL = 0.8 (< 1.5)
    ‚úó NO PASA filtros
    ‚Üí Descartado (continue)
  
  ... (11,902 tickers m√°s)
  
  05:30:01.100 - Completa procesamiento
  Resultado: 337 tickers filtrados
  
  05:30:01.150 - Ordena por score
  05:30:01.200 - Asigna ranks (1-337)
```

### T+1250ms: Guarda cach√©s

```
05:30:01.250 - Guarda en memoria
  self.last_filtered_tickers = [337 tickers]
  
05:30:01.300 - Guarda en Redis
  SET scanner:filtered_complete:PRE_MARKET = [337 tickers]
```

### T+1300ms - T+1450ms: Categoriza

```
05:30:01.300 - Categoriza 337 tickers
  
  Procesa cada categor√≠a:
  
  GAPPERS_UP:
    - Filtra: 97 tickers con change >= 2%
    - Ordena por change_percent desc
    - Top 20: [CRBU +204%, MSPR +122%, RAY +53%, ...]
  
  WINNERS:
    - Filtra: 45 tickers con change >= 5%
    - Ordena por change_percent desc
    - Top 20: [CRBU, MSPR, RAY, ...]
  
  ANOMALIES:
    - Filtra: 89 tickers con RVOL >= 3.0
    - Ordena por RVOL desc
    - Top 20: [MSPR 2787x, CRBU 2493x, ...]
  
  ... (8 categor√≠as m√°s)
  
  05:30:01.450 - Categorizaci√≥n completa
```

### T+1450ms - T+1480ms: Calcula y emite deltas

```
05:30:01.450 - Para cada categor√≠a:

  GAPPERS_UP:
    Old (seq 119): [CRBU, MSPR, RAY, SMX, LGCL, ...]
    New (seq 120): [CRBU, MSPR, RAY, SMX, ETHZW, ...]
    
    Deltas calculados:
    - remove: LGCL
    - add: ETHZW (rank 6)
    - rerank: RAY (3‚Üí4)
    
    05:30:01.455 - XADD stream:ranking:deltas
    Mensaje: {
      list: "gappers_up",
      sequence: 120,
      timestamp: "2025-11-03T05:30:01.455Z",
      changes: 3,
      deltas: "[{remove LGCL}, {add ETHZW}, {rerank RAY}]"
    }
    
    05:30:01.460 - SET scanner:category:gappers_up = [20 tickers]
    05:30:01.462 - SET scanner:sequence:gappers_up = 120
    05:30:01.465 - last_rankings["gappers_up"] = [20 tickers]
  
  WINNERS:
    [mismo proceso...]
  
  ... (7 categor√≠as m√°s)

05:30:01.480 - Emisi√≥n de deltas completa
```

### T+1500ms: Guarda en TimescaleDB

```
05:30:01.500 - Batch INSERT
  INSERT INTO scan_results VALUES (...), (...), ... (337 rows)
  
05:30:01.520 - Insert completo
```

### T+1530ms: Finaliza

```
05:30:01.530 - Actualiza estad√≠sticas
  total_scans = 120
  total_tickers_scanned = 1,428,600
  total_tickers_filtered = 40,440
  
05:30:01.540 - Retorna ScannerResult

05:30:01.550 - Log final:
  "üîç Discovery scan completed: filtered_count=337, total_scanned=11905, duration_sec=1.55"
```

---

## üåä PROPAGACI√ìN A WEBSOCKET Y FRONTEND

### WebSocket Server (consume deltas)

```javascript
// Consumer corriendo en background

XREAD stream:ranking:deltas BLOCK 1000

Mensaje recibido:
{
  list: "gappers_up",
  sequence: 120,
  deltas: "[...]"
}

// Broadcast a todos los clientes suscritos
for (connection of connections.values()) {
  if (connection.subscriptions.has("gappers_up")) {
    
    // Verificar sequence
    clientSeq = connection.sequence_numbers.get("gappers_up")
    
    if (sequence > clientSeq + 1) {
      // Gap ‚Üí Enviar snapshot completo
      sendSnapshot(connection, "gappers_up")
    } else {
      // Enviar delta
      ws.send(JSON.stringify({
        type: "delta",
        list: "gappers_up",
        sequence: 120,
        changes: JSON.parse(deltas)
      }))
      
      connection.sequence_numbers.set("gappers_up", 120)
    }
  }
}
```

### Frontend (aplica deltas)

```typescript
// Recibe mensaje WebSocket
message = {
  type: "delta",
  list: "gappers_up",
  sequence: 120,
  changes: [
    {type: "remove", ticker: "LGCL"},
    {type: "add", ticker: {...}, rank: 6},
    {type: "rerank", ticker: "RAY", old_rank: 3, new_rank: 4}
  ]
}

// Verificar sequence
if (message.sequence !== lastSequence + 1) {
  requestResync()
  return
}

// Aplicar cada delta
setTickers(prevTickers => {
  let newTickers = [...prevTickers]
  
  for (delta of message.changes) {
    if (delta.type === "remove") {
      newTickers = newTickers.filter(t => t.symbol !== "LGCL")
      console.log("‚ûñ Removed LGCL")
    }
    
    if (delta.type === "add") {
      newTickers.push(delta.ticker)
      console.log("‚ûï Added ETHZW at rank 6")
    }
    
    if (delta.type === "rerank") {
      const idx = newTickers.findIndex(t => t.symbol === "RAY")
      newTickers[idx].rank = 4
      console.log("‚ÜïÔ∏è Reranked RAY: 3 ‚Üí 4")
    }
  }
  
  return newTickers
})

setLastSequence(120)
```

---

## üìä M√âTRICAS DE PERFORMANCE

### Por Scan (cada 30 segundos)

```
Lectura snapshot:           100ms
Procesamiento 11,905:       900ms
  ‚îú‚îÄ MGET metadatas:        50ms
  ‚îú‚îÄ Filtrado inline:       800ms
  ‚îî‚îÄ Ordenar + rank:        50ms
Categorizaci√≥n:             150ms
  ‚îú‚îÄ 337 tickers ‚Üí 8 cats:  100ms
  ‚îî‚îÄ Ordenar √ó 8:           50ms
C√°lculo deltas:             80ms (8 categor√≠as)
Emisi√≥n deltas:             70ms (8 XADD)
Guardar Redis:              100ms (16 SETs)
Guardar TimescaleDB:        100ms (1 batch INSERT)

TOTAL: ~1,500ms (1.5 segundos)
```

### Throughput

```
Tickers procesados/segundo: 11,905 / 1.5 = 7,937 tickers/s
Tickers filtrados/scan: 337
Categor√≠as generadas/scan: 8
Rankings guardados/scan: 8 √ó 20 = 160 tickers
Deltas emitidos/scan: 8 mensajes
Rows en BD/scan: 337
```

---

## üîß PUNTOS CR√çTICOS Y POSIBLES PROBLEMAS

### 1. Metadatas Faltantes

**Problema**:
```
Si metadata:ticker:MNOV no existe en Redis:
‚Üí L√≠nea 275: if not metadata: continue
‚Üí MNOV se descarta aunque tenga buen RVOL
```

**Soluci√≥n Actual**:
- Warmup carga 11,899 metadatas con TTL 24h
- NO se borran despu√©s (bug arreglado)

**Verificaci√≥n**:
```bash
# Ver cu√°ntas metadatas hay
docker exec tradeul_redis redis-cli KEYS "metadata:ticker:*" | wc -l

# Ver metadata espec√≠fica
docker exec tradeul_redis redis-cli GET "metadata:ticker:MNOV"
```

### 2. Timestamp Synchronization

**Problema Potencial**:
```
Si Scanner procesa MUY r√°pido:
- Analytics guarda snapshot a las 05:30:00.500
- Scanner lo procesa a las 05:30:00.600
- Data Ingest actualiza a las 05:30:01.000
- Scanner corre otra vez a las 05:30:30.000
- Snapshot sigue siendo 05:30:01.000
- ¬øProcesa el mismo snapshot?
```

**Soluci√≥n Implementada**:
```python
if snapshot_timestamp == self.last_snapshot_timestamp:
    return []  # No procesar
```

**Verificaci√≥n**:
```bash
# Logs deben mostrar "Reading complete enriched snapshot" solo cuando hay nuevo
docker logs tradeul_scanner | grep "Reading complete" | uniq -c
# No debe repetirse el mismo timestamp
```

### 3. Sequence Number Gaps

**Problema Potencial**:
```
Scanner emite: seq 118, 119, 120
WebSocket recibe: 118, 119, (se pierde 120)
Cliente recibe: 118, 119, 121
‚Üí Gap detectado
```

**Soluci√≥n**:
- Frontend detecta gap (121 !== 119 + 1)
- Solicita resync
- WebSocket env√≠a snapshot completo
- Cliente se sincroniza

**Verificaci√≥n**:
```
Frontend logs:
‚úÖ "üîÑ Received DELTA {sequence: 119}"
‚úÖ "üîÑ Received DELTA {sequence: 120}"
‚ùå "‚ö†Ô∏è Sequence gap detected (expected 120, got 122)"
‚Üí Si ves esto, hay problema en WebSocket
```

### 4. Categor√≠as Vac√≠as

**Problema**:
```
En premarket temprano:
- No hay tickers con change >= 5% ‚Üí winners = []
- No hay new_highs ‚Üí new_highs = []
```

**Comportamiento**:
```python
L√≠nea 232 (scanner_categories.py):
if ranked:  # Solo si hay tickers
    results[category.value] = ranked
```

Si categor√≠a vac√≠a ‚Üí NO se incluye en results

**Verificaci√≥n**:
```bash
# Ver qu√© categor√≠as tienen datos
docker exec tradeul_redis redis-cli KEYS "scanner:category:*"
```

---

## üìù CONCLUSIONES

### ¬øQu√© hace Scanner POST-filtrado?

1. ‚úÖ **Guarda en 3 cach√©s** (memoria, Redis completo, Redis por categor√≠a)
2. ‚úÖ **Categoriza** en 8-11 listas diferentes
3. ‚úÖ **Calcula deltas** comparando con iteraci√≥n anterior
4. ‚úÖ **Emite deltas** al stream para WebSocket
5. ‚úÖ **Guarda snapshots** en Redis para recovery
6. ‚úÖ **Persiste en BD** para hist√≥rico
7. ‚úÖ **Actualiza estad√≠sticas** para monitoring

### Servicios que consumen sus datos

```
CONSUMERS:
‚úÖ WebSocket Server ‚Üí Lee deltas y snapshots
‚úÖ Frontend ‚Üí Recibe v√≠a WebSocket
‚úÖ API Gateway ‚Üí Lee cach√© completo
‚úÖ TimescaleDB ‚Üí Para queries hist√≥ricos

NO CONSUMERS:
‚ùå Analytics (no lee de Scanner)
‚ùå Data Ingest (no lee de Scanner)
```

### Metadata: ¬øQui√©n crea qu√©?

```
Historical Service CREA:
- ticker_metadata (BD)
- metadata:ticker:{symbol} (Redis)
Campos: sector, industry, market_cap, avg_volume_30d

Scanner AGREGA:
- metadata.gaps (calculado)
- metadata.gap_size_classification
- metadata.gap_metrics

Scanner GUARDA TODO en:
- scan_results.metadata (JSONB)
```

---

**Documento completo**: Scanner Post-Filtrado  
**Versi√≥n**: Post-refactor snapshot cache  
**Estado**: Producci√≥n funcional
